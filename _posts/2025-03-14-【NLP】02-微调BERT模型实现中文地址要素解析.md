---
title: 【NLP】02-微调BERT模型实现中文地址要素解析
author: Autumn
date: 2025-03-14 +0800
categories: [NLP, NLP项目]
tags: [NLP, 命名实体识别, 微调]
---

## 项目简介
使用人工标注数据集微调bert-base-chinese实现中文地址要素解析。
项目地址：[nlp02](https://github.com/AuTuMnnn458/NLP_project/tree/main/nlp02)

### 模型
bert-base-chinese
- 地址：[bert-base-chinese(hugging face)](https://huggingface.co/google-bert/bert-base-chinese)
- 简介：经典BERT，采用的是base-chinese版本。

### 数据
- 地址：[数据集(github)](https://github.com/AuTuMnnn458/NLP_project/tree/main/nlp02/data)
- 简介：标注数据集由训练集（train.txt）、验证集（dev.txt）和标注规范（标注规范.pdf ）组成。其中训练集中包含8856条人工标注的地址数据，验证集中包含1970条人工标注的地址数据。所有地址数据按照字符进行分割，并将每个字符添加上标签。标签由【词位置(BIOES)-地址要素】组成，共56种情况。
	- B：Begin，代表实体片段的开始
	- I ：Inside，代表实体片段的中间
	- E：End，代表实体片段的结束
	- S：Single，代表实体片段的单个字
	- O：Other，代表字符不为任何实体


## 项目特点/难点
1. 数据集的预处理阶段比较麻烦，这个任务是一个以token为单位的任务，所以需要先从句子层面划分数据，再在每个句子中从字的层面划分。
2. 本次训练我用GPU跑了10个epoch，从val-loss中可以观察到模型有点过拟合了。如果复现的话可以不选择checkpoint-2770做模型推理，实验上看模型对于一些比较奇怪的地址的推理结果表现不好。在epoch=5的时候val-loss来到最低，可以考虑使用其他checkpoint。
3. 后面还写了挺多的代码，其实只是为了优化输出结果。把地址要素转换为能看懂的术语。



