---
title: 【NLP】02-微调BERT模型实现中文地址要素解析
author: Autumn
date: 2025-04-02 +0800
categories: [NLP, NLP项目]
tags: [NLP, 命名实体识别, 微调]
---

## 项目简介
使用人工标注数据集微调bert-base-chinese实现中文地址要素解析。
项目地址：[nlp02](https://github.com/AuTuMnnn458/NLP_project/tree/main/nlp02)

### 模型
bert-base-chinese
- 地址：[bert-base-chinese(hugging face)](https://huggingface.co/google-bert/bert-base-chinese)
- 简介：经典BERT，采用的是base-chinese版本。

### 数据
- 地址：[数据集(github)](https://github.com/AuTuMnnn458/NLP_project/tree/main/nlp02/data)
- 简介：标注数据集由训练集（train.txt）、验证集（dev.txt）和标注规范（标注规范.pdf ）组成。其中训练集中包含8856条人工标注的地址数据，验证集中包含1970条人工标注的地址数据。所有地址数据按照字符进行分割，并将每个字符添加上标签。标签由【词位置(BIOES)-地址要素】组成，共56种情况。
	- B：Begin，代表实体片段的开始
	- I ：Inside，代表实体片段的中间
	- E：End，代表实体片段的结束
	- S：Single，代表实体片段的单个字
	- O：Other，代表字符不为任何实体


## 项目特点/难点
1. 数据集的预处理阶段比较麻烦，这个任务是一个以token为单位的任务，所以需要先从句子层面划分数据，再在每个句子中从字的层面划分。
2. 本次训练我用GPU跑了10个epoch，从val-loss中可以观察到模型有点过拟合了。如果复现的话可以不选择checkpoint-2770做模型推理，实验上看模型对于一些比较奇怪的地址的推理结果表现不好。在epoch=5的时候val-loss来到最低，可以考虑使用其他checkpoint。
3. 后面还写了挺多的代码，其实只是为了优化输出结果。把地址要素转换为能看懂的术语。
4. （2025.4.2更新）重新复现了代码之后发现有几个问题：
  - (1)使用的`checkpoint-2770`在示例`朝阳区小关北里000-0号`上表现良好，但**不代表**在其他大部分例子上都表现良好。有时候模型的预测结果会出现有`B-road`，`I-road`却没有`E-road`，即某个命名实体在模型预测下有开头，有中间但没有结尾，这使得结果展示是非常困难格式化的。我在尝试使用gradio搭前端优化输出的时候发现了上面这个问题，这是个比较严重的问题，原因是模型训练不充分导致的。
  - (2)针对上述的问题，我重新跑了一遍模型，也是训练`epoch=10`，各种指标都有不同，基本上差不多，但是在示例`朝阳区小关北里000-0号`预测已经出现问题，会出现预测结果为:`['E-prov', 'I-community', 'B-community', 'I-houseno', 'S-community', 'S-community', 'B-prov', 'I-road', 'E-community', 'E-community', 'E-community', 'E-community', 'B-houseno', 'E-road', 'E-distance']`。可以看到基本上不符合`B-I-E(Begin, interval, end)`的结构，格式化输出也基本不可能了因为无法前后匹配。所以后续也不再更新该项目。



