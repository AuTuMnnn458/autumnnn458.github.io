---
title: ã€NLPã€‘å¾®è°ƒæ¨¡å‹çš„ä»£ç èŒƒå¼
author: Autumn
date: 2025-03-19 +0800
categories: [NLP, NLPæ‚é¡¹]
tags: [NLP]
---

## ç®€ä»‹
NLPçš„å¾®è°ƒæ¨¡å‹é¡¹ç›®ä»£ç éµå¾ªç€ä¸€å®šçš„èŒƒå¼ï¼Œæ¯ä¸ªéƒ¨åˆ†æœ‰å¤šç§ä»£ç é€‰æ‹©ï¼Œæœ¬æ–‡å°†å…·ä½“ä»‹ç»è¿™äº›èŒƒå¼ã€‚

æˆ‘æ¯”è¾ƒå¸¸ç”¨çš„NLPæ•°æ®å¤„ç†ä¹ æƒ¯æ˜¯ï¼š
1. åŸå§‹æ•°æ®ï¼ˆ`dataframe/dict, text+label`ï¼‰é€šè¿‡`Dataset.from_dict`è½¬æ¢ä¸º`dataset`æ•°æ®ç±»å‹ã€‚
2. è°ƒç”¨æ¨¡å‹å¯¹åº”çš„`tokenizer`ã€‚
3. ä½¿ç”¨`tokenizer`ç¼–å†™`collate_fn`ï¼Œä½¿ç”¨`torch.utils.data`çš„`DataLoader`å®šä¹‰`dataloader`ç±»ã€‚
4. è°ƒç”¨é¢„è®­ç»ƒæ¨¡å‹`from transformers import ...`æˆ–è‡ªå·±å®šä¹‰æ¨¡å‹ç±»`class Model()...`ï¼ˆè‡ªå®šçš„æ¨¡å‹ä¹Ÿå¯ä»¥æ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼Œåœ¨`def __init__`ä¸­å®šä¹‰`self.pretrained = ...`å³å¯ã€‚ï¼‰
5. æ¨¡å‹è®­ç»ƒã€‚ä»¥å‰æ˜¯è‡ªå·±å†™æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ï¼Œç›®å‰æ›´å€¾å‘äºä½¿ç”¨`trainer`æ¥åšäº†ã€‚
6. æ¨¡å‹è¯„ä¼°ã€‚ä»¥å‰åœ¨è®­ç»ƒä¸­ä¼šé¡ºå¸¦å†™è¯„ä¼°æŒ‡æ ‡ï¼Œç°åœ¨æ›´å€¾å‘äºè°ƒç”¨`evaluate`åº“çš„è¯„ä¼°æŒ‡æ ‡äº†ã€‚
7. æ¨¡å‹æ¨ç†ã€‚ä»¥å‰æ˜¯è‡ªå·±å†™ï¼Œç°åœ¨`pipeline`æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©ã€‚

## æ•°æ®é¢„å¤„ç†
æ¥æºæ•°æ®å¯èƒ½æ˜¯å¤šç§æ ¼å¼çš„ï¼Œç”±äºæ¨¡å‹è®­ç»ƒåªæ¥æ”¶`dataset`ç±»æˆ–`dataloader`ç±»ï¼Œæ‰€ä»¥æ— è®ºæ˜¯å“ªç§æ ¼å¼ï¼Œæœ€åéƒ½éœ€è¦å°è£…è¿›`datasets`åº“çš„`Dataset`ç±»ä¸­ã€‚

### ç›´æ¥åŠ è½½æˆæ•°æ®é›†ç±»
ä¸€èˆ¬ä»`hugging face`ä¸Šä¸‹è½½çš„æ•°æ®éƒ½æ˜¯`.arrow`æ ¼å¼ï¼Œæˆ–`.json`æ ¼å¼ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨`datasets`åº“çš„`load_dataset`ç›´æ¥åŠ è½½æ•°æ®ã€‚åŠ è½½å¾—åˆ°çš„æ•°æ®ç›´æ¥ä¸º`Dataset`ç±»ã€‚

ä¿®æ”¹`load_dataset`çš„`path`å‚æ•°ä¹Ÿå¯ä»¥ç›´æ¥åŠ è½½æœ¬åœ°æ•°æ®ã€‚

ç¤ºä¾‹ä»£ç ï¼š
```python
from datasets import load_dataset
dataset = load_dataset(path = 'lansinuote/ChnSentiCorp')
# ä»hugging faceä¸‹è½½æ•°æ®ï¼Œå­˜æ”¾è·¯å¾„C:\Users\Username\.cache\huggingface\datasets, 
# Usernameä¸ºä½ çš„ç”µè„‘çš„ç”¨æˆ·å
# ä¿®æ”¹pathä¸ºæœ¬åœ°è·¯å¾„ï¼Œå³å¯åŠ è½½æœ¬åœ°çš„æ•°æ®é›†
print(dataset)
```
è¾“å‡ºï¼š
```python
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 9600
    })
    validation: Dataset({
        features: ['text', 'label'],
        num_rows: 1200
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 1200
    })
})
```

### å­—å…¸æ ¼å¼çš„æ•°æ®è½¬æ¢ä¸ºæ•°æ®é›†ç±»
å¦‚æœæ•°æ®é›†æ˜¯`.csv`æˆ–`DataFrame`ç­‰æ ¼å¼çš„æ•°æ®ï¼Œé‚£ä¹ˆä½¿ç”¨è¯¥æ–¹æ³•å¯ä»¥å°†æ•°æ®å°è£…ä¸ºæ•°æ®é›†ç±»ã€‚è°ƒç”¨`datasets`åº“çš„`Dataset`ï¼Œè°ƒç”¨`Dataset.from_dict()`ï¼Œå³å¯å®Œæˆå­—å…¸æ ¼å¼åˆ°æ•°æ®é›†ç±»çš„è½¬æ¢ã€‚

ç¤ºä¾‹ï¼š
```python
from datasets import Dataset
...
model_inputs['labels'] = labels['input_ids']    
model_inputs['translation'] = [{'en':e, 'zh':c} for c, e in zip(inputs, targets)]
model_inputs = Dataset.from_dict(model_inputs)
...
```


## è°ƒç”¨Tokenizer/åˆ†è¯å™¨
ä½¿ç”¨`AutoTokenizer`æ¥è°ƒç”¨å¯¹åº”æ¨¡å‹çš„`tokenizer`ã€‚è°ƒç”¨çš„å‚æ•°ä¸`load_dataset`ä¸åé¢çš„æ¨¡å‹è°ƒç”¨`AutoModelForCasualLM`ï¼ˆç­‰ï¼‰ä¸€æ ·ï¼Œç»™å®šä¸€ä¸ª`path`ï¼Œä»£ç é¦–å…ˆæ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰æ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰å°±ä¼šåˆ°`hugging face`ä¸‹è½½ã€‚

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')
# å­˜æ”¾è·¯å¾„ C:\Users\Username\.cache\huggingface\hub
```

å®ä¾‹åŒ–ä¹‹åå¯ä»¥å¯¹æ•°æ®è¿›è¡Œ`padding`, `truncation`, `max_length`ç­‰çš„æ•°æ®å¤„ç†ï¼Œè€Œè¿™ä¸ªå¤„ç†ä¸å…¶ä»–çš„æ­¥éª¤ä¹‹é—´çš„é¡ºåºæ˜¯æ¯”è¾ƒçµæ´»çš„ã€‚å…·ä½“ä¾‹å­çœ‹ä¸‹ä¸€å°èŠ‚ã€‚

## æ•°æ®åŠ è½½å™¨
å¯ä»¥ä½¿ç”¨`torch`åº“ä¸­çš„`torch.utils.data.DataLoader`ï¼š
```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')
# å®šä¹‰æ•°æ®é›†ä¸­çš„æ•°æ®æ•´ç†å‡½æ•°
def collate_fn(data):
    text = [i['text'] for i in data]
    label = [i['label'] for i in data]
    
    # æ–‡å­—ç¼–ç 
    data = tokenizer(text,
                    padding = True,
                    truncation = True,
                    max_length = 500,
                    return_tensors = 'pt',
                    return_token_type_ids = False)
    
    data['label'] = torch.LongTensor(label) # è®¾ç½®label,æœ¬èº«å°±æ˜¯æ•°æ®
    return data

from torch.utils.data import DataLoader
loader = DataLoader(dataset['train'], 
				   # è¿™é‡Œçš„datasetæ˜¯æ²¡æœ‰ç»è¿‡tokenizeråˆ†è¯çš„ï¼ŒæŠŠåˆ†è¯éƒ¨åˆ†å†™åœ¨collate_fnä¸­
                   batch_size=8,
                   shuffle = True,
                   drop_last = True,
                   collate_fn = collate_fn)
```

ä¹Ÿå¯ä»¥ä½¿ç”¨`transformers`åº“ä¸­çš„`DataCollatorForSeq2Seq`ï¼šï¼ˆåº”è¯¥è¿˜æœ‰å¾ˆå¤šå…¶ä»–ç±»å‹çš„ï¼‰
```python
from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(
    tokenizer,  # åˆ†è¯å™¨
    model,  # é¢„è®­ç»ƒæ¨¡å‹
    label_pad_token_id=-100,   # paddingå¯¹åº”çš„idï¼Œé»˜è®¤-100
)
```

## ä¸€äº›éœ€è¦æ³¨æ„çš„ç‚¹
åœ¨æ¨¡å‹æ­£å¼è®­ç»ƒä»¥å‰ï¼Œæ•°æ®çš„é¢„å¤„ç†ï¼ˆæ•°æ®åŠ è½½ä¸é¢„å¤„ç†ã€è°ƒç”¨tokenizerã€å°è£…æˆdatasetsç±»ã€å°è£…æˆdataloaderç±»ï¼‰æ˜¯éå¸¸çµæ´»çš„ï¼Œæ²¡æœ‰ä¸¥æ ¼çš„é¡ºåºã€‚

å› ä¸ºåœ¨`torch.utils.data.DataLoader`ä¸­æœ‰ä¸€ä¸ªå‚æ•°å«`collate_fn`ï¼Œå¯ä»¥è‡ªå®šä¹‰ä¸€ä¸ª`collate_fn`å‡½æ•°ï¼ŒåŒ…å«`tokenizer`çš„è½¬æ¢ã€‚æ„å‘³ç€ä¸€ç§æ•°æ®é¢„å¤„ç†çš„æ–¹å¼æ˜¯ï¼š
- åŸå§‹æ•°æ®ï¼ˆ`dataframe, text+label`ï¼‰â†’ä½¿ç”¨`Dataset.from_dict`å°è£…æˆ`dataset`â†’ä½¿ç”¨`torch.utils.data.DataLoader`å°è£…ä¸º`dataloader`ç±»

å½“ç„¶ä¹Ÿå¯ä»¥è¿™æ ·åšï¼š
- åŸå§‹æ•°æ®ï¼ˆ`dataframe, text+label`ï¼‰â†’ä½¿ç”¨`tokenizer`å¤„ç†æ•°æ®â†’æŠŠå¤„ç†å¥½çš„æ•°æ®å°è£…ï¼ˆé€šè¿‡`from_dict`ï¼‰ä¸º`dataset`â†’ä½¿ç”¨`transformers.DataCollatorForSeq2Seq`è½¬æ¢ä¸ºæ•°æ®åŠ è½½å™¨

æ³¨ï¼šåŸå§‹æ•°æ®ï¼ˆ`dataframe, text+label`ï¼‰æ˜¯ä¸€ä¸ªç®€å†™ï¼Œæœ‰ä¸€äº›æœºå™¨ç¿»è¯‘çš„ä»»åŠ¡ä¸­`text`å’Œ`label`å…¶å®éƒ½æ˜¯ä¸€æ®µ`text`ã€‚

## é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
ä¸åŠ è½½`tokenizer`ä¸€æ ·ï¼Œç»™å®šå‚æ•°ä¼šé¦–å…ˆæœç´¢æœ¬åœ°ï¼Œç„¶åå†æœç´¢`hugging face`çš„æ¨¡å‹è¿›è¡Œåœ¨çº¿ä¸‹è½½ã€‚ä¸‹é¢ç»™å‡ ä¸ªä¾‹å­ã€‚

```python
model_ckpt = data_path + '/Helsinki-NLP--opus-mt-zh-en/'

from transformers import AutoModelForSeq2SeqLM
model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)
```

å½“ç„¶ï¼Œå¾®è°ƒæ¨¡å‹ä¸€èˆ¬ä¸æ˜¯å…¨é‡å¾®è°ƒï¼Œå¦‚æƒ³å†»ç»“é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œè‡ªå·±å†åŠ ä¸€ä¸ªFCå±‚ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰æ¨¡å‹ã€‚å¦‚ä¸‹ï¼šï¼ˆä¸‹é¢è¿™ä¸ªç±»æ˜¯æŠŠ`forward`ä¹Ÿå†™äº†ï¼Œåé¢æˆ‘æ›´å¤šçš„ä½¿ç”¨`trainer`æ¥åšã€‚ï¼‰

```python
#å®šä¹‰æ¨¡å‹
class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()

        #åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        from transformers import AutoModel
        self.pretrained = AutoModel.from_pretrained(
            'google-bert/bert-base-chinese')

        self.fc = torch.nn.Linear(in_features=768, out_features=2)

    def forward(self, input_ids, attention_mask, label=None):
        #ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æŠ½å–æ•°æ®ç‰¹å¾
        with torch.no_grad():
            last_hidden_state = self.pretrained(
                input_ids=input_ids,
                attention_mask=attention_mask).last_hidden_state

        #åªå–ç¬¬0ä¸ªè¯çš„ç‰¹å¾åšåˆ†ç±»,è¿™å’Œbertæ¨¡å‹çš„è®­ç»ƒæ–¹å¼æœ‰å…³,æ­¤å¤„ä¸å±•å¼€
        last_hidden_state = last_hidden_state[:, 0]

        #å¯¹æŠ½å–çš„ç‰¹å¾åªå–ç¬¬ä¸€ä¸ªå­—çš„ç»“æœåšåˆ†ç±»å³å¯
        out = self.fc(last_hidden_state).softmax(dim=1)

        #è®¡ç®—loss
        loss = None
        if label is not None:
            loss = torch.nn.functional.cross_entropy(out, label)

        return loss, out
```

## å¾®è°ƒè®­ç»ƒ
å¯ä»¥è‡ªå®šä¹‰å‡½æ•°ï¼šï¼ˆä»£ç æ¥ä¸Šï¼‰
```python
model = Model()
#æ‰§è¡Œè®­ç»ƒ
def train():
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    for i, data in enumerate(loader):
        loss, out = model(**data)

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if i % 10 == 0:
            out = out.argmax(dim=1)
            acc = (out == data.label).sum().item() / len(data.label)
            print(i, len(loader), loss.item(), acc)

        if i == 100:
            break

train()
```

ä¹Ÿå¯ä»¥ä½¿ç”¨`trainer`ï¼Œéœ€è¦é€šè¿‡`transformers`çš„`Seq2SeqTrainingArguments`ï¼ˆæˆ–å…¶ä»–åº“ï¼‰ç»™å®šå‚æ•°è®¾ç½®ï¼Œè¿˜éœ€è¦é€šè¿‡`evaluate`ç»™å®šè¯„ä¼°æŒ‡æ ‡ï¼ˆæˆ–è‡ªå®šä¹‰å‡½æ•°ï¼‰ï¼š

```python
from transformers import AutoModelForSeq2SeqLM
model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)

from evaluate import load
# pip install evaluate
# pip install sacrebleu
sacrebleu_path = r'D:\model\web\nlp01\huggingface\modules\evaluate_modules\metrics\evaluate-metric--sacrebleu\28676bf65b4f88b276df566e48e603732d0b4afd237603ebdf92acaacf5be99b\sacrebleu.py'
metric = load(sacrebleu_path)

from transformers import Seq2SeqTrainer
trainer = Seq2SeqTrainer(
    model,
    args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train() # å¾®è°ƒè®­ç»ƒ
```

ğŸ’¡å¾ˆç¥å¥‡çš„æ˜¯ï¼Œè¿™ä¸ªéƒ¨åˆ†æˆ‘è¿æ¥äº†wandbï¼Œå¯ä»¥åœ¨æœ¬åœ°çœ‹åˆ°å„ç§æŒ‡æ ‡å›¾ã€‚æ­£å¸¸æ¥è¯´ä½¿ç”¨`trainer`ä¹Ÿä¼šæœ‰ä¸€ä¸ªè¡¨æ ¼å±•ç¤º`training loss`å’Œ`val loss`ï¼Œä»¥åŠè‡ªå·±ç»™å®šçš„`metrics`çš„æŒ‡æ ‡ã€‚


