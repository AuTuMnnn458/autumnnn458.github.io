---
title: 【NLP】微调模型的代码范式
author: Autumn
date: 2025-03-19 +0800
categories: [NLP, NLP杂项]
tags: [NLP]
---

## 简介
NLP的微调模型项目代码遵循着一定的范式，每个部分有多种代码选择，本文将具体介绍这些范式。

我比较常用的NLP数据处理习惯是：
1. 原始数据（`dataframe/dict, text+label`）通过`Dataset.from_dict`转换为`dataset`数据类型。
2. 调用模型对应的`tokenizer`。
3. 使用`tokenizer`编写`collate_fn`，使用`torch.utils.data`的`DataLoader`定义`dataloader`类。
4. 调用预训练模型`from transformers import ...`或自己定义模型类`class Model()...`（自定的模型也可以是预训练模型，在`def __init__`中定义`self.pretrained = ...`即可。）
5. 模型训练。以前是自己写整个训练过程，目前更倾向于使用`trainer`来做了。
6. 模型评估。以前在训练中会顺带写评估指标，现在更倾向于调用`evaluate`库的评估指标了。
7. 模型推理。以前是自己写，现在`pipeline`是个不错的选择。

## 数据预处理
来源数据可能是多种格式的，由于模型训练只接收`dataset`类或`dataloader`类，所以无论是哪种格式，最后都需要封装进`datasets`库的`Dataset`类中。

### 直接加载成数据集类
一般从`hugging face`上下载的数据都是`.arrow`格式，或`.json`格式，可以直接使用`datasets`库的`load_dataset`直接加载数据。加载得到的数据直接为`Dataset`类。

修改`load_dataset`的`path`参数也可以直接加载本地数据。

示例代码：
```python
from datasets import load_dataset
dataset = load_dataset(path = 'lansinuote/ChnSentiCorp')
# 从hugging face下载数据，存放路径C:\Users\Username\.cache\huggingface\datasets, 
# Username为你的电脑的用户名
# 修改path为本地路径，即可加载本地的数据集
print(dataset)
```
输出：
```python
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 9600
    })
    validation: Dataset({
        features: ['text', 'label'],
        num_rows: 1200
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 1200
    })
})
```

### 字典格式的数据转换为数据集类
如果数据集是`.csv`或`DataFrame`等格式的数据，那么使用该方法可以将数据封装为数据集类。调用`datasets`库的`Dataset`，调用`Dataset.from_dict()`，即可完成字典格式到数据集类的转换。

示例：
```python
from datasets import Dataset
...
model_inputs['labels'] = labels['input_ids']    
model_inputs['translation'] = [{'en':e, 'zh':c} for c, e in zip(inputs, targets)]
model_inputs = Dataset.from_dict(model_inputs)
...
```


## 调用Tokenizer/分词器
使用`AutoTokenizer`来调用对应模型的`tokenizer`。调用的参数与`load_dataset`与后面的模型调用`AutoModelForCasualLM`（等）一样，给定一个`path`，代码首先检查本地是否有文件，如果没有就会到`hugging face`下载。

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')
# 存放路径 C:\Users\Username\.cache\huggingface\hub
```

实例化之后可以对数据进行`padding`, `truncation`, `max_length`等的数据处理，而这个处理与其他的步骤之间的顺序是比较灵活的。具体例子看下一小节。

## 数据加载器
可以使用`torch`库中的`torch.utils.data.DataLoader`：
```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')
# 定义数据集中的数据整理函数
def collate_fn(data):
    text = [i['text'] for i in data]
    label = [i['label'] for i in data]
    
    # 文字编码
    data = tokenizer(text,
                    padding = True,
                    truncation = True,
                    max_length = 500,
                    return_tensors = 'pt',
                    return_token_type_ids = False)
    
    data['label'] = torch.LongTensor(label) # 设置label,本身就是数据
    return data

from torch.utils.data import DataLoader
loader = DataLoader(dataset['train'], 
				   # 这里的dataset是没有经过tokenizer分词的，把分词部分写在collate_fn中
                   batch_size=8,
                   shuffle = True,
                   drop_last = True,
                   collate_fn = collate_fn)
```

也可以使用`transformers`库中的`DataCollatorForSeq2Seq`：（应该还有很多其他类型的）
```python
from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(
    tokenizer,  # 分词器
    model,  # 预训练模型
    label_pad_token_id=-100,   # padding对应的id，默认-100
)
```

## 一些需要注意的点
在模型正式训练以前，数据的预处理（数据加载与预处理、调用tokenizer、封装成datasets类、封装成dataloader类）是非常灵活的，没有严格的顺序。

因为在`torch.utils.data.DataLoader`中有一个参数叫`collate_fn`，可以自定义一个`collate_fn`函数，包含`tokenizer`的转换。意味着一种数据预处理的方式是：
- 原始数据（`dataframe, text+label`）→使用`Dataset.from_dict`封装成`dataset`→使用`torch.utils.data.DataLoader`封装为`dataloader`类

当然也可以这样做：
- 原始数据（`dataframe, text+label`）→使用`tokenizer`处理数据→把处理好的数据封装（通过`from_dict`）为`dataset`→使用`transformers.DataCollatorForSeq2Seq`转换为数据加载器

注：原始数据（`dataframe, text+label`）是一个简写，有一些机器翻译的任务中`text`和`label`其实都是一段`text`。

## 预训练模型加载
与加载`tokenizer`一样，给定参数会首先搜索本地，然后再搜索`hugging face`的模型进行在线下载。下面给几个例子。

```python
model_ckpt = data_path + '/Helsinki-NLP--opus-mt-zh-en/'

from transformers import AutoModelForSeq2SeqLM
model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)
```

当然，微调模型一般不是全量微调，如想冻结预训练模型参数，自己再加一个FC层，也可以自定义模型。如下：（下面这个类是把`forward`也写了，后面我更多的使用`trainer`来做。）

```python
#定义模型
class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()

        #加载预训练模型
        from transformers import AutoModel
        self.pretrained = AutoModel.from_pretrained(
            'google-bert/bert-base-chinese')

        self.fc = torch.nn.Linear(in_features=768, out_features=2)

    def forward(self, input_ids, attention_mask, label=None):
        #使用预训练模型抽取数据特征
        with torch.no_grad():
            last_hidden_state = self.pretrained(
                input_ids=input_ids,
                attention_mask=attention_mask).last_hidden_state

        #只取第0个词的特征做分类,这和bert模型的训练方式有关,此处不展开
        last_hidden_state = last_hidden_state[:, 0]

        #对抽取的特征只取第一个字的结果做分类即可
        out = self.fc(last_hidden_state).softmax(dim=1)

        #计算loss
        loss = None
        if label is not None:
            loss = torch.nn.functional.cross_entropy(out, label)

        return loss, out
```

## 微调训练
可以自定义函数：（代码接上）
```python
model = Model()
#执行训练
def train():
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    for i, data in enumerate(loader):
        loss, out = model(**data)

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if i % 10 == 0:
            out = out.argmax(dim=1)
            acc = (out == data.label).sum().item() / len(data.label)
            print(i, len(loader), loss.item(), acc)

        if i == 100:
            break

train()
```

也可以使用`trainer`，需要通过`transformers`的`Seq2SeqTrainingArguments`（或其他库）给定参数设置，还需要通过`evaluate`给定评估指标（或自定义函数）：

```python
from transformers import AutoModelForSeq2SeqLM
model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)

from evaluate import load
# pip install evaluate
# pip install sacrebleu
sacrebleu_path = r'D:\model\web\nlp01\huggingface\modules\evaluate_modules\metrics\evaluate-metric--sacrebleu\28676bf65b4f88b276df566e48e603732d0b4afd237603ebdf92acaacf5be99b\sacrebleu.py'
metric = load(sacrebleu_path)

from transformers import Seq2SeqTrainer
trainer = Seq2SeqTrainer(
    model,
    args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train() # 微调训练
```

💡很神奇的是，这个部分我连接了wandb，可以在本地看到各种指标图。正常来说使用`trainer`也会有一个表格展示`training loss`和`val loss`，以及自己给定的`metrics`的指标。


