---
title: 【ML】机器学习传统算法简要介绍
author: Autumn
date: 2025-03-27 +0800
categories: [机器学习, ML杂项]
tags: [机器学习]
---


# 数据划分
通过`sklearn`的`train_test_split`对数据进行训练集和验证集的划分。

```python
# 假设数据的特征集为X，标签集为y
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_split = 0.2, random_state = 42)
```

# 模型评估
分类问题和回归问题有不同的评估指标。回归问题一般使用`mean_squared_error(MSE), mean_absolute_error(MAE), r2_score`，分类问题一般使用`classification_report`。

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
# 假设真实值为y_val, 预测值为y_pred
print(r2_score(y_val, y_pred))
print(mean_squared_error(y_val, y_pred))
print(mean_absolute_error(y_val, y_pred))
```

```python
from sklearn.metrics import classification_report
print(classification_report(y_val, y_pred))
```
当然，分类问题中有大量指标，如`f1_score`, `accuracy_score`, `recall_score`, `precision_score`, 而这些指标都会包含在`classification_report`中，所以一般只看`classification_report`就可以了。

# 线性模型

线性模型假设特征`x`与标签`y`具有线性关系，建立模型`y = kx + b`，模型给定目标函数（一般是预测值和真实值的mse），通过`梯度下降法`或`最小二乘法`的学习参数`k`和`b`。

线性模型的局限性非常大，因为它对于数据做了一个很强的假设。在线性模型的基础上有许多变种，如`岭回归`、`Lasso回归`。这两个模型都是在目标函数上加入正则项，提升效果非常有限。具体看[sklearn中文文档-线性模型](https://scikit-learn.org.cn/view/4.html#1.1.1%20%E6%99%AE%E9%80%9A%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95)。

```python
# 假设数据已经划分好
from sklearn.linear_model import LinearRegression
lr = LinearRegression(fit_intercept = True) # 无太多可调参数
lr.fit(X_train, y_train)

result = lr.predict(X_val)

from sklearn.metrics import r2_score
r2_score(y_val,result)
```

# 逻辑回归

逻辑回归也算是线性回归的一种变种。在`广义线性模型`中`y = g(wx)`的`g`选择`sigmoid`函数，即
$$y = \frac{1}{1+e^{-(w^{T}X+b)}},$$
则
$$ln \frac{y}{1-y} = w^{T}X +b.$$

```python
from sklearn.linear_model import LogisticRegression,LogisticRegressionCV
lg = LogisticRegression(penalty='l1',
                        dual=False,
                        tol=0.0001,
                        C=5.0,
                        fit_intercept=True,
                        intercept_scaling=1,
                        class_weight=None,
                        solver='liblinear') 
lg.fit(X_train, y_train)
```

# 决策树
基于一定的指标（ID3-信息增益，C4.5-信息增益率，CART-基尼指数），在训练集中学习规则，在测试集中应用规则。

CART树可以做回归和分类，其他的只能做分类。
```python
from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor # 默认都是CART了
model = DecisionTreeClassifier(
    criterion="gini",  # "gini"CART树, "entropy"ID3决策树
    max_depth=6, # 树的最大深度
    min_samples_split=2, # 子节点至少N个样本
    min_samples_leaf=1,  # 叶子节点至少N个样本
    max_features=None,   # 最大特征个数
  )
```


# KNN
k近邻算法。按照近朱者赤近墨者黑的原则，新的样本的类别由离它最近k个样本的类别决定。“最近”意味着计算距离的方法的多样性，“决定”意味着投票方法的多样性。

KNN可以做分类也可以做回归。回归只需要最后算均值就可以了。

KNN现在基本不用了，一个是因为其他机器学习算法（主要是集成学习）的效果好，另一个是它的计算成本高，它需要计算新样本到所有训练样本的距离并存储下来。

```python
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
knn = KNeighborsClassifier(n_neighbors=5,  # 邻居个数K
                     metric='manhattan' # 距离计算方式，sklearn.neighbors.VALID_METRICS['brute']
                    )
```

# 贝叶斯算法
假设标签为`y`，特征集为`x`，特征分量为`x_1,x_2,...`，根据贝叶斯公式
$$P(y|x) = \frac{P(y)P(x|y)}{P(x)},$$
其中`P(y)`可以直接算，`P(x)`对于二分类问题来说一样，忽略不算。而`P(x|y)`根据条件概率公式来算：
$$P(x|y) = P(x_1|y)P(x_2|y)...$$

这里有个假设，假设所有特征之间互相是独立的。离散状态下，`P(x_k|y)`等于它的频率；连续状态下计算高斯分布的概率就可以了。

贝叶斯算法本质就是在做统计，根据先验概率来直接计算后验概率。有些问题，如文本分类，算法的速度会比SVM快一点点，但是在`MultinomialNB`下都计算很慢。

```python
from sklearn.naive_bayes import GaussianNB, MultinomialNB
gnb = GaussianNB()
```

# SVM
支持向量机。把数据从低维线性不可分通过核函数映射到高维的线性可分。（一般是低维到高维，其实没有严格证明过）

```python
# 支持向量机算法
from sklearn.svm import LinearSVC,LinearSVR,SVC,SVR
# LinearSVR、LinearSVC  # 核函数为线性的支持向量机， LinearSVC速度快
svc = SVC(
        C=1.0,          # 惩罚系数
        kernel='sigmoid',   # 核函数linear', 'poly', 'rbf', 'sigmoid'
        degree=3,       # poly核函数的多项式
        gamma='scale',  # 对'poly', 'rbf', 'sigmoid'核函数的处理：{'scale', 'auto'}, 取数值,越大，间隔越小
        tol=0.001,      # 模型停止训练的误差标准
        cache_size=200, # 模型训练的内存M
        class_weight=None, # 类别权重，当类别不均衡使用100：1。{0:1, 1:30}
        verbose=False,  # 是不是打印训练的信息
        max_iter=1000,  # 最大训练次数
        random_state=None) # 随机初始化参数的随机种子

svc.fit(X_train, y_train)
```


# Bagging&Random Forest
Bagging = Bootstrap aggregating，打包法。核心在于通过bootstrap每次取不一样的训练集，在同一个模型上训练得出不同的模型参数已经模型结果，最后综合看结果。如果这个模型选择决策树，那么这一整个模型称为随机森林Random Forest。这些模型可以做回归和分类。

```python
from sklearn.ensemble import BaggingRegressor, BaggingClassifier
from sklearn.svm import SVC
bag_svm = BaggingClassifier(
    base_estimator=SVC(),  # 基础学习器（模型），换成DecisionTreeClassifier相当于随机森林
    n_estimators=10,       # 生成多少个模型
    max_samples=1.0,       # 使用多少比例的样本0-1， 大于1的整数表示用多少个
    max_features=1.0,      # 使用多少比例的特征0-1， 大于1的整数表示用多少个
    bootstrap=True,        # 有放回数据抽样
    bootstrap_features=True, # 有放回特征抽样
)

bag_svm.fit(X_train, y_train)
```

```python
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
rf = RandomForestClassifier(n_estimators=100, # 多少颗决策树
                            max_depth=5, # 树的最大深度
                           )
rf.fit(X_train, y_train)
```

# Boosting
提升算法。AdaBoost是更改数据集的分布，使得后续的学习器学习错题更多。Gradiant Boosting Decision Tree(GBDT)是用决策树作为学习器，学习残差。

```python
from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor
from sklearn.tree import DecisionTreeClassifier
adb = AdaBoostClassifier(base_estimator= DecisionTreeClassifier(),
                        n_estimators=20,
                        learning_rate = 0.1, # 学习率
                        algorithm='SAMME')
```

```python
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
gbdt = GradientBoostingClassifier(n_estimators=100, random_state=42)
```

提升算法有非常强大的能力，使得融合后的模型一定比单个模型的效果好。除了上面提到的2种，更现代一点的模型还有`HistGradientBoosting`，`LightGBM`，`XGBoost`，`CatBoost`。其中最为广泛使用，且效果较好的是`XGBoost`。

```python
from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor
from lightgbm import LGBMClassifier, LGBMRegressor # pip install lightgbm
from xgboost import XGBClassifier,XGBRegressor # pip install xgboost
from catboost import CatBoostClassifier # pip install catboost
```

# Stacking
堆叠。把前一个学习器的结果当作输入传到下一个学习器。有点像神经网络，但是一般`stacking`用的都不是一个或一类学习器。

```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

from sklearn.ensemble import StackingClassifier, StackingRegressor
base_model = [('lg', LogisticRegression()),
             ('dtree', DecisionTreeClassifier()),
             ('rf', RandomForestClassifier()),
              ('knn', KNeighborsClassifier()),
              ('nb',GaussianNB())
             ]

final_model = GradientBoostingClassifier()

st = StackingClassifier(estimators=base_model, # 第一层初级学习器
                       final_estimator=final_model, # 第二层的次级学习器
                        stack_method='auto' # 初级学习器输出的内容：'predict_proba','decision_function','predict'
                       )
```

